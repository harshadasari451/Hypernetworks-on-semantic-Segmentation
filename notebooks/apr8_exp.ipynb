{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/CAMPUS/hdasari/Hypernetworks_stevens\n"
     ]
    }
   ],
   "source": [
    "cd /home/CAMPUS/hdasari/Hypernetworks_stevens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1378390/3870493577.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expert_checkpoint = torch.load(unet_path)\n",
      "/tmp/ipykernel_1378390/3870493577.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  global_checkpoint = torch.load(global_model_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNetTinyTwoBlock(\n",
       "  (softconv): Conv2d(1, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (inc): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (drop1): DropOut()\n",
       "  (down2): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (drop2): DropOut()\n",
       "  (up3): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (drop5): DropOut()\n",
       "  (up4): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): OutConv(\n",
       "    (conv): Conv2d(8, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models.Unet_model import UNet, UNetTinyTwoBlock\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "unet_path = \"/home/CAMPUS/sgangadh1/projects/rl-batt-seg-snapshot-jan-2024/src/outputs/rerun-battery-01/unet_model_checkpoint_finetuned.pt\"\n",
    "global_model_path = \"/home/CAMPUS/hdasari/HyperNetworks/model_checkpoints_AAI_paper/unet_tiny_two_block_model_checkpoint_199_final.pt\"\n",
    "\n",
    "expert_model_unet = UNet(n_channels=1, n_classes=3)\n",
    "expert_model_unet = expert_model_unet.to(device)\n",
    "\n",
    "global_model = UNetTinyTwoBlock(n_channels=1, n_classes=3)\n",
    "global_model = global_model.to(device)\n",
    "\n",
    "\n",
    "expert_checkpoint = torch.load(unet_path)\n",
    "global_checkpoint = torch.load(global_model_path)\n",
    "\n",
    "expert_model_unet.load_state_dict(expert_checkpoint['model_state_dict'])\n",
    "expert_model_unet.eval()\n",
    "\n",
    "global_model.load_state_dict(global_checkpoint['model_state_dict'])\n",
    "global_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from src.utils.hyp_input import hyp_input\n",
    "from src.utils.get_boundary_pixels import get_boundary_pixels\n",
    "from src.utils.extract_patch import extract_patch\n",
    "\n",
    "\n",
    "class Battery_unet_hyp_data(Dataset):\n",
    "    def __init__(self, image_dir,label_dir,expert_model,small_model, device, mask_function=hyp_input, get_boundaries=get_boundary_pixels, get_patch=extract_patch, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.mask_function = mask_function\n",
    "        self.get_boundaries = get_boundaries\n",
    "        self.get_patch = get_patch\n",
    "        self.transform = transform\n",
    "        self.expert_model = expert_model\n",
    "        self.small_model = small_model\n",
    "        self.device = device\n",
    "        \n",
    "        self.image_files = sorted(Path(image_dir).glob('*.png'))\n",
    "        self.label_files = sorted(Path(label_dir).glob('*.png'))\n",
    "        assert len(self.image_files) == len(self.label_files), \"Number of image and label files must be the same!\"\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]\n",
    "        label_path = self.label_files[idx]\n",
    "        label = Image.open(label_path)\n",
    "\n",
    "        mask_transform = transforms.Compose([\n",
    "            transforms.Lambda(lambda x: x.convert('L') if x.mode != 'L' else x),\n",
    "            transforms.Lambda(lambda x: np.array(x, dtype=np.float32) / 255.0),\n",
    "            transforms.Lambda(lambda x: np.where((x > 0) & (x < 1.0), 2.0, x)),\n",
    "            transforms.Lambda(lambda x: torch.as_tensor(x.copy()).long()),\n",
    "        ])\n",
    "        label_tensor = mask_transform(label) \n",
    "\n",
    "        # Load and transform images\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        img_ndarray = np.asarray(image)\n",
    "        img_ndarray = img_ndarray[np.newaxis, ...]  # Add channel dimension [1, H, W]\n",
    "        image_tensor = torch.as_tensor(img_ndarray / 255.0).float().contiguous()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_img = image_tensor.unsqueeze(0).to(self.device)  # Add batch dimension\n",
    "            expert_label_tensor = self.expert_model(input_img)\n",
    "            small_label_tensor = self.small_model(input_img)\n",
    "        expert_label_tensor = expert_label_tensor.squeeze(0).cpu().type(torch.long)\n",
    "        small_label_tensor = small_label_tensor.squeeze(0).cpu().type(torch.long)\n",
    "\n",
    "        # print(f\"expert_label_tensor shape: {expert_label_tensor.shape}\")\n",
    "        # print(f\"small_label_tensor shape: {small_label_tensor.shape}\")\n",
    "\n",
    "        \n",
    "\n",
    "        _, H, W = image_tensor.shape\n",
    "\n",
    "        # Get key pixels and masked image\n",
    "        key_pixels, expert_patches, global_patches = self.mask_function(expert_label_tensor, small_label_tensor)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"masked_img shape: {expert_patches.shape}\")\n",
    "        # print(f\"global_patches shape: {global_patches.shape}\")\n",
    "\n",
    "        all_patches = []\n",
    "        all_labels = []\n",
    "\n",
    "        mismatch = 0\n",
    "\n",
    "        for x, y in key_pixels:\n",
    "            boundary_pixels = self.get_boundaries(x, y)\n",
    "\n",
    "            patches = []\n",
    "            labels = []\n",
    "\n",
    "            for bx, by in boundary_pixels:\n",
    "                if bx < 0 or by < 0 or bx >= H or by >= W:\n",
    "                    patches.append(torch.zeros((1, 9, 9), dtype=torch.long))\n",
    "                    labels.append(255)\n",
    "                    mismatch += 1\n",
    "                else:\n",
    "                    patches.append(self.get_patch(image_tensor, bx, by))\n",
    "                    labels.append(label_tensor[bx, by])  # Get label ID\n",
    "\n",
    "            all_patches.append(torch.stack(patches))  # Shape: (max_boundaries, C, H, W)\n",
    "            all_labels.append(torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        all_patches = torch.stack(all_patches)  # Shape: (num_key_pixels, max_boundaries, C, H, W)\n",
    "        all_labels = torch.stack(all_labels)  # Shape: (num_key_pixels, max_boundaries)\n",
    "\n",
    "        return all_patches, global_patches, expert_patches, all_labels, mismatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_patches length: 16\n",
      "all_patches shape: torch.Size([16, 3, 40, 1, 9, 9])\n",
      "global shape: torch.Size([16, 3, 371])\n",
      "expert shape: torch.Size([16, 3, 251])\n",
      "all_labels shape: torch.Size([16, 3, 40])\n",
      "Mismatches: tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 17])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_dir = \"/home/CAMPUS/sgangadh1/projects/rl-batt-seg-snapshot-jan-2024/data/battery_2/train_images\"\n",
    "label_dir = \"/home/CAMPUS/sgangadh1/projects/rl-batt-seg-snapshot-jan-2024/data/battery_2/train_label\"\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = Battery_unet_hyp_data(image_dir, label_dir, expert_model_unet, global_model,device)\n",
    "\n",
    "\n",
    "# Iterate through the DataLoader and inspect the output\n",
    "def display_first_batch_shape(dataset, batch_size=1):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    for batch in dataloader:\n",
    "        all_patches, global_patches, expert_patches, all_labels, mismatch = batch\n",
    "        print(f\"all_patches length: {len(all_patches)}\") \n",
    "        print(f\"all_patches shape: {(all_patches.shape)}\") # Expected: (batch_size, K, 40, C, H, W)\n",
    "        print(f\"global shape: {global_patches.shape}\")  # Expected: (batch_size, k,concatednated_patch[9*9+8])\n",
    "        print(f\"expert shape: {expert_patches.shape}\")  # Expected: (batch_size, K,2)\n",
    "        print(f\"all_labels shape: {all_labels.shape}\")  # Expected: (batch_size, K, 40)\n",
    "        print(f\"Mismatches: {mismatch}\")\n",
    "        break\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "       \n",
    "display_first_batch_shape(train_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self, f_size=3, z_dim=64, out_size=16, in_size=16):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.f_size = f_size\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "\n",
    "        # Define parameters and move them to GPU\n",
    "        self.w1 = Parameter(torch.fmod(torch.randn((16, self.out_size * self.in_size * self.f_size * self.f_size)), 2))\n",
    "        self.b1 = Parameter(torch.fmod(torch.randn((self.out_size * self.in_size * self.f_size * self.f_size)), 2))\n",
    "\n",
    "        self.w_global_img = Parameter(torch.fmod(torch.randn((371, 64)), 2))\n",
    "        self.b_global_img = Parameter(torch.fmod(torch.randn((64)), 2))\n",
    "\n",
    "        self.w_expert_img = Parameter(torch.fmod(torch.randn((251, 64)), 2))\n",
    "        self.b_expert_img = Parameter(torch.fmod(torch.randn((64)), 2))\n",
    "\n",
    "        self.dense_w = Parameter(torch.fmod(torch.randn((144, 16)), 2))\n",
    "        self.dense_b = Parameter(torch.fmod(torch.randn((16)), 2))\n",
    "        \n",
    "        self.w2 = Parameter(torch.fmod(torch.randn((self.z_dim, self.in_size)), 2))\n",
    "        self.b2 = Parameter(torch.fmod(torch.randn(self.in_size), 2))\n",
    "\n",
    "    def forward(self, z, expert_patch, global_patch):\n",
    "\n",
    "        h_z = torch.matmul(z, self.w2) + self.b2\n",
    "        h_z = torch.tanh(h_z)\n",
    "        \n",
    "        # y_global_img = torch.squeeze(global_patch, dim=0).float()\n",
    "        y_global_prime = torch.matmul(global_patch, self.w_global_img) + self.b_global_img\n",
    "        y_global_prime = torch.tanh(y_global_prime)\n",
    "\n",
    "        \n",
    "        # y_expert_img = torch.squeeze(expert_patch, dim=0).float()\n",
    "        y_expert_prime = torch.matmul(expert_patch, self.w_expert_img) + self.b_expert_img\n",
    "        h_img_trans = torch.cat([y_global_prime, y_expert_prime, h_z], dim = 0)\n",
    "        h_prime = torch.matmul(h_img_trans, self.dense_w) + self.dense_b  \n",
    "        h_prime = torch.tanh(h_prime)      \n",
    "        h_final = torch.matmul(h_prime, self.w1) + self.b1\n",
    "        h_final = torch.tanh(h_final)\n",
    "        kernel = h_final.view(self.out_size, self.in_size, self.f_size, self.f_size)\n",
    "\n",
    "        return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IdentityLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_size=16, out_size=16, downsample=False):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "        if downsample:\n",
    "            self.stride1 = 2\n",
    "            self.reslayer = nn.Conv2d(in_channels=self.in_size, out_channels=self.out_size, stride=1, kernel_size=3, padding=1) # some mistake. need to fix it\n",
    "        else:\n",
    "            self.stride1 = 1\n",
    "            self.reslayer = IdentityLayer()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_size)\n",
    "        self.bn2 = nn.BatchNorm2d(out_size)\n",
    "\n",
    "    def forward(self, x, conv1_w, conv2_w):\n",
    "        x = x\n",
    "        residual = self.reslayer(x)\n",
    "        # print(f\"Residual shape: {residual.shape}\")\n",
    "\n",
    "        out = F.relu(self.bn1(F.conv2d(x, conv1_w, stride=1, padding=1)), inplace=True)\n",
    "        # print(f\"Conv1 shape: {out.shape}\")\n",
    "        out = self.bn2(F.conv2d(out, conv2_w, padding=1))\n",
    "\n",
    "        out += residual\n",
    "        # print(f\"Out shape: {out.shape}\")\n",
    "\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, z_num, z_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.z_list = nn.ParameterList()\n",
    "        self.z_num = z_num\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        h,k = self.z_num\n",
    "\n",
    "        for i in range(h):\n",
    "            for j in range(k):\n",
    "                self.z_list.append(Parameter(torch.fmod(torch.randn(self.z_dim).cuda(), 2)))\n",
    "\n",
    "    def forward(self, hyper_net, expert_patch, global_patch):\n",
    "        ww = []\n",
    "        h, k = self.z_num\n",
    "        for i in range(h):\n",
    "            w = []\n",
    "            for j in range(k):\n",
    "                w.append(hyper_net(self.z_list[i*k + j],expert_patch, global_patch))\n",
    "            ww.append(torch.cat(w, dim=1))\n",
    "        return torch.cat(ww, dim=0)\n",
    "\n",
    "\n",
    "class PrimaryNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim=64):\n",
    "        super(PrimaryNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.hope = HyperNetwork(z_dim=self.z_dim)\n",
    "\n",
    "        self.zs_size = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1],\n",
    "                        [2, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2],\n",
    "                        [4, 2], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4], [4, 4]]\n",
    "\n",
    "        self.filter_size = [[16,16], [16,16], [16,16], [16,16], [16,16], [16,16], [16,32], [32,32], [32,32], [32,32],\n",
    "                            [32,32], [32,32], [32,64], [64,64], [64,64], [64,64], [64,64], [64,64]]\n",
    "\n",
    "        self.res_net = nn.ModuleList()\n",
    "\n",
    "        for i in range(18):\n",
    "            down_sample = False\n",
    "            if i > 5 and i % 6 == 0:\n",
    "                down_sample = True\n",
    "            self.res_net.append(ResNetBlock(self.filter_size[i][0], self.filter_size[i][1], downsample=down_sample))\n",
    "\n",
    "        self.zs = nn.ModuleList()\n",
    "\n",
    "        for i in range(36):\n",
    "            self.zs.append(Embedding(self.zs_size[i], self.z_dim))\n",
    "\n",
    "        self.global_avg = nn.AvgPool2d(9)\n",
    "        self.final = nn.Linear(64,3) # change for number of classes\n",
    "\n",
    "    def forward(self, x,expert_patch,global_patch):\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        for i in range(18):\n",
    "            w1 = self.zs[2*i](self.hope, expert_patch, global_patch)\n",
    "            w2 = self.zs[2*i+1](self.hope, expert_patch, global_patch)            \n",
    "            x = self.res_net[i](x, w1, w2)\n",
    "            \n",
    "\n",
    "        x = self.global_avg(x)\n",
    "        x= x.squeeze(-1).squeeze(-1)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([40, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Assuming CUDA is available\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = PrimaryNetwork(z_dim=64).to(device)\n",
    "\n",
    "# Dummy input: Batch of 4 RGB images of size 72x72 (adjust as per your needs)\n",
    "x = torch.randn(40,1,9,9).to(device)\n",
    "\n",
    "# Dummy expert_patch and global_patch\n",
    "# Adjust dimensions as required by your HyperNetwork\n",
    "expert_patch = torch.randn(251).to(device)\n",
    "global_patch = torch.randn(371).to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(x, expert_patch, global_patch)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Should be [4, 3] if final layer has 20 output classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "PrimaryNetwork                           [40, 3]                   --\n",
       "├─Conv2d: 1-1                            [40, 16, 9, 9]            160\n",
       "├─BatchNorm2d: 1-2                       [40, 16, 9, 9]            32\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-1                    [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-4                      [16, 16, 3, 3]            82,464\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-2                    [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-6                      [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-3                  [40, 16, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-1           [40, 16, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-2             [40, 16, 9, 9]            32\n",
       "│    │    └─BatchNorm2d: 3-3             [40, 16, 9, 9]            32\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-4                    [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-9                      [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-5                    [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-11                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-6                  [40, 16, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-4           [40, 16, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-5             [40, 16, 9, 9]            32\n",
       "│    │    └─BatchNorm2d: 3-6             [40, 16, 9, 9]            32\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-7                    [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-14                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-8                    [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-16                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-9                  [40, 16, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-7           [40, 16, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-8             [40, 16, 9, 9]            32\n",
       "│    │    └─BatchNorm2d: 3-9             [40, 16, 9, 9]            32\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-10                   [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-19                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-11                   [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-21                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-12                 [40, 16, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-10          [40, 16, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-11            [40, 16, 9, 9]            32\n",
       "│    │    └─BatchNorm2d: 3-12            [40, 16, 9, 9]            32\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-13                   [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-24                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-14                   [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-26                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-15                 [40, 16, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-13          [40, 16, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-14            [40, 16, 9, 9]            32\n",
       "│    │    └─BatchNorm2d: 3-15            [40, 16, 9, 9]            32\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-16                   [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-29                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-17                   [16, 16, 3, 3]            64\n",
       "├─HyperNetwork: 1-31                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-18                 [40, 16, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-16          [40, 16, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-17            [40, 16, 9, 9]            32\n",
       "│    │    └─BatchNorm2d: 3-18            [40, 16, 9, 9]            32\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-19                   [32, 16, 3, 3]            128\n",
       "├─HyperNetwork: 1-34                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-35                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-20                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-37                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-38                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-39                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-40                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-21                 [40, 32, 9, 9]            --\n",
       "│    │    └─Conv2d: 3-19                 [40, 32, 9, 9]            4,640\n",
       "│    │    └─BatchNorm2d: 3-20            [40, 32, 9, 9]            64\n",
       "│    │    └─BatchNorm2d: 3-21            [40, 32, 9, 9]            64\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-22                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-43                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-44                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-45                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-46                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-23                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-48                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-49                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-50                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-51                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-24                 [40, 32, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-22          [40, 32, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-23            [40, 32, 9, 9]            64\n",
       "│    │    └─BatchNorm2d: 3-24            [40, 32, 9, 9]            64\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-25                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-54                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-55                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-56                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-57                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-26                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-59                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-60                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-61                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-62                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-27                 [40, 32, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-25          [40, 32, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-26            [40, 32, 9, 9]            64\n",
       "│    │    └─BatchNorm2d: 3-27            [40, 32, 9, 9]            64\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-28                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-65                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-66                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-67                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-68                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-29                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-70                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-71                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-72                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-73                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-30                 [40, 32, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-28          [40, 32, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-29            [40, 32, 9, 9]            64\n",
       "│    │    └─BatchNorm2d: 3-30            [40, 32, 9, 9]            64\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-31                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-76                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-77                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-78                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-79                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-32                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-81                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-82                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-83                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-84                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-33                 [40, 32, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-31          [40, 32, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-32            [40, 32, 9, 9]            64\n",
       "│    │    └─BatchNorm2d: 3-33            [40, 32, 9, 9]            64\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-34                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-87                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-88                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-89                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-90                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-35                   [32, 32, 3, 3]            256\n",
       "├─HyperNetwork: 1-92                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-93                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-94                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-95                     [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-36                 [40, 32, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-34          [40, 32, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-35            [40, 32, 9, 9]            64\n",
       "│    │    └─BatchNorm2d: 3-36            [40, 32, 9, 9]            64\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-37                   [64, 32, 3, 3]            512\n",
       "├─HyperNetwork: 1-98                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-99                     [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-100                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-101                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-102                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-103                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-104                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-105                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-38                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-107                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-108                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-109                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-110                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-111                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-112                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-113                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-114                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-115                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-116                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-117                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-118                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-119                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-120                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-121                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-122                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-39                 [40, 64, 9, 9]            --\n",
       "│    │    └─Conv2d: 3-37                 [40, 64, 9, 9]            18,496\n",
       "│    │    └─BatchNorm2d: 3-38            [40, 64, 9, 9]            128\n",
       "│    │    └─BatchNorm2d: 3-39            [40, 64, 9, 9]            128\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-40                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-125                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-126                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-127                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-128                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-129                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-130                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-131                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-132                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-133                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-134                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-135                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-136                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-137                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-138                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-139                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-140                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-41                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-142                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-143                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-144                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-145                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-146                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-147                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-148                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-149                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-150                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-151                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-152                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-153                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-154                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-155                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-156                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-157                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-42                 [40, 64, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-40          [40, 64, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-41            [40, 64, 9, 9]            128\n",
       "│    │    └─BatchNorm2d: 3-42            [40, 64, 9, 9]            128\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-43                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-160                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-161                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-162                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-163                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-164                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-165                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-166                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-167                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-168                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-169                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-170                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-171                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-172                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-173                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-174                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-175                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-44                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-177                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-178                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-179                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-180                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-181                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-182                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-183                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-184                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-185                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-186                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-187                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-188                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-189                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-190                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-191                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-192                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-45                 [40, 64, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-43          [40, 64, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-44            [40, 64, 9, 9]            128\n",
       "│    │    └─BatchNorm2d: 3-45            [40, 64, 9, 9]            128\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-46                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-195                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-196                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-197                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-198                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-199                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-200                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-201                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-202                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-203                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-204                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-205                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-206                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-207                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-208                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-209                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-210                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-47                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-212                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-213                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-214                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-215                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-216                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-217                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-218                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-219                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-220                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-221                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-222                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-223                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-224                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-225                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-226                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-227                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-48                 [40, 64, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-46          [40, 64, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-47            [40, 64, 9, 9]            128\n",
       "│    │    └─BatchNorm2d: 3-48            [40, 64, 9, 9]            128\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-49                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-230                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-231                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-232                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-233                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-234                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-235                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-236                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-237                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-238                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-239                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-240                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-241                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-242                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-243                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-244                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-245                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-50                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-247                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-248                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-249                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-250                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-251                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-252                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-253                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-254                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-255                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-256                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-257                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-258                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-259                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-260                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-261                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-262                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-51                 [40, 64, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-49          [40, 64, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-50            [40, 64, 9, 9]            128\n",
       "│    │    └─BatchNorm2d: 3-51            [40, 64, 9, 9]            128\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-52                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-265                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-266                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-267                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-268                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-269                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-270                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-271                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-272                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-273                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-274                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-275                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-276                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-277                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-278                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-279                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-280                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-281                      --                        (recursive)\n",
       "│    └─Embedding: 2-53                   [64, 64, 3, 3]            1,024\n",
       "├─HyperNetwork: 1-282                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-283                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-284                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-285                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-286                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-287                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-288                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-289                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-290                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-291                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-292                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-293                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-294                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-295                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-296                    [16, 16, 3, 3]            (recursive)\n",
       "├─HyperNetwork: 1-297                    [16, 16, 3, 3]            (recursive)\n",
       "├─ModuleList: 1-298                      --                        (recursive)\n",
       "│    └─ResNetBlock: 2-54                 [40, 64, 9, 9]            --\n",
       "│    │    └─IdentityLayer: 3-52          [40, 64, 9, 9]            --\n",
       "│    │    └─BatchNorm2d: 3-53            [40, 64, 9, 9]            128\n",
       "│    │    └─BatchNorm2d: 3-54            [40, 64, 9, 9]            128\n",
       "├─AvgPool2d: 1-299                       [40, 64, 1, 1]            --\n",
       "├─Linear: 1-300                          [40, 3]                   195\n",
       "==========================================================================================\n",
       "Total params: 124,163\n",
       "Trainable params: 124,163\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 75.60\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 42.62\n",
       "Params size (MB): 0.43\n",
       "Estimated Total Size (MB): 43.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = PrimaryNetwork(z_dim=64).to(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "summary(model, input_data=(x, expert_patch, global_patch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for unet and unet_tiny models : cpu\n",
      "Using device: cuda:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3001128/2413518608.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expert_checkpoint = torch.load(unet_path)\n",
      "/tmp/ipykernel_3001128/2413518608.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  global_checkpoint = torch.load(global_model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting from epoch 0.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Detected more unique values in `target` than expected. Expected only 3 but found 4 in `target`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 138\u001b[0m\n\u001b[1;32m    136\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(np\u001b[38;5;241m.\u001b[39marray(all_true_labels)\u001b[38;5;241m.\u001b[39mflatten(), np\u001b[38;5;241m.\u001b[39marray(all_preds)\u001b[38;5;241m.\u001b[39mflatten(), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    137\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(np\u001b[38;5;241m.\u001b[39marray(all_true_labels)\u001b[38;5;241m.\u001b[39mflatten(), np\u001b[38;5;241m.\u001b[39marray(all_preds)\u001b[38;5;241m.\u001b[39mflatten(), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m iou \u001b[38;5;241m=\u001b[39m \u001b[43mjaccard_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_true_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmulticlass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[1;32m    141\u001b[0m log_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmar28_exp_battery/training_log.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/ENTER/envs/hypernetworks/lib/python3.10/site-packages/torchmetrics/functional/classification/jaccard.py:368\u001b[0m, in \u001b[0;36mjaccard_index\u001b[0;34m(preds, target, task, threshold, num_classes, num_labels, average, ignore_index, validate_args, zero_division)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_classes, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_classes` is expected to be `int` but `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(num_classes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was passed.`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmulticlass_jaccard_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m ClassificationTask\u001b[38;5;241m.\u001b[39mMULTILABEL:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_labels, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/ENTER/envs/hypernetworks/lib/python3.10/site-packages/torchmetrics/functional/classification/jaccard.py:237\u001b[0m, in \u001b[0;36mmulticlass_jaccard_index\u001b[0;34m(preds, target, num_classes, average, ignore_index, validate_args, zero_division)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_args:\n\u001b[1;32m    236\u001b[0m     _multiclass_jaccard_index_arg_validation(num_classes, ignore_index, average)\n\u001b[0;32m--> 237\u001b[0m     \u001b[43m_multiclass_confusion_matrix_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_confusion_matrix_format(preds, target, ignore_index)\n\u001b[1;32m    239\u001b[0m confmat \u001b[38;5;241m=\u001b[39m _multiclass_confusion_matrix_update(preds, target, num_classes)\n",
      "File \u001b[0;32m~/ENTER/envs/hypernetworks/lib/python3.10/site-packages/torchmetrics/functional/classification/confusion_matrix.py:291\u001b[0m, in \u001b[0;36m_multiclass_confusion_matrix_tensor_validation\u001b[0;34m(preds, target, num_classes, ignore_index)\u001b[0m\n\u001b[1;32m    289\u001b[0m num_unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(torch\u001b[38;5;241m.\u001b[39munique(t, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_unique_values \u001b[38;5;241m>\u001b[39m check_value:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected more unique values in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` than expected. Expected only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_unique_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in `target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Detected more unique values in `target` than expected. Expected only 3 but found 4 in `target`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models.Unet_model import UNet, UNetTinyTwoBlock\n",
    "from src.data_loaders.Battery_unet_hyp_dataloader import Battery_unet_hyp_data\n",
    "from src.models.primaryNet import PrimaryNetwork\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from torchmetrics.functional import jaccard_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device_1 = torch.device(\"cpu\")\n",
    "print(f\"Using device for unet and unet_tiny models : {device_1}\")\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "unet_path = \"/home/CAMPUS/sgangadh1/projects/rl-batt-seg-snapshot-jan-2024/src/outputs/rerun-battery-01/unet_model_checkpoint_finetuned.pt\"\n",
    "global_model_path = \"/home/CAMPUS/hdasari/HyperNetworks/model_checkpoints_AAI_paper/unet_tiny_two_block_model_checkpoint_199_final.pt\"\n",
    "\n",
    "expert_model_unet = UNet(n_channels=1, n_classes=3)\n",
    "expert_model_unet = expert_model_unet.to(device_1)\n",
    "\n",
    "global_model = UNetTinyTwoBlock(n_channels=1, n_classes=3)\n",
    "global_model = global_model.to(device_1)\n",
    "\n",
    "\n",
    "expert_checkpoint = torch.load(unet_path)\n",
    "global_checkpoint = torch.load(global_model_path)\n",
    "\n",
    "expert_model_unet.load_state_dict(expert_checkpoint['model_state_dict'])\n",
    "expert_model_unet.eval()\n",
    "\n",
    "global_model.load_state_dict(global_checkpoint['model_state_dict'])\n",
    "global_model.eval()\n",
    "\n",
    "\n",
    "image_dir = \"/home/CAMPUS/sgangadh1/projects/rl-batt-seg-snapshot-jan-2024/data/battery_2/train_images\"\n",
    "label_dir = \"/home/CAMPUS/sgangadh1/projects/rl-batt-seg-snapshot-jan-2024/data/battery_2/train_label\"\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = Battery_unet_hyp_data(image_dir, label_dir, expert_model_unet, global_model,device_1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "model = PrimaryNetwork()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "start_epoch = 0\n",
    "end_epoch = 100\n",
    "num_batches = 1\n",
    "\n",
    "checkpoint_path = \"/home/CAMPUS/hdasari/HyperNetworks/unet_hyp_apr9/checkpoint_epoch_0.pth\"  \n",
    "\n",
    "# Load checkpoint if available\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']  \n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0  # Start from scratch if no checkpoint found\n",
    "    print(\"No checkpoint found. Starting from epoch 0.\")\n",
    "\n",
    "# Initialize lists to store true and predicted labels\n",
    "# Training loop\n",
    "total_true_labels = []\n",
    "total_preds = []\n",
    "loss_per_epoch = []\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    loss_epoch = 0\n",
    "    loss_per_batch = []\n",
    "    all_preds = []\n",
    "    all_true_labels = []\n",
    "    num_left_count = 0  # Reset for each epoch\n",
    "    data_len = 0  # Track the number of valid batches\n",
    "\n",
    "    for all_patches, global_patches, expert_patches, all_labels,mismatch in train_loader:\n",
    "\n",
    "        for batch in range(len(all_patches)):\n",
    "            batch_patches = all_patches[batch].to(device)\n",
    "            batch_global_patches = global_patches[batch].to(device)\n",
    "            batch_expert_patches = expert_patches[batch].to(device)\n",
    "            batch_labels = all_labels[batch].to(device)\n",
    "            mismatch_count = mismatch[batch]\n",
    "\n",
    "            \n",
    "            batch_loss = 0\n",
    "            count = 0  # Count of skipped pixels\n",
    "\n",
    "            for patch in range(len(batch_patches)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch_patches[patch],batch_expert_patches[patch],batch_global_patches[patch])\n",
    "                loss = criterion(output, batch_labels[patch])\n",
    "\n",
    "                batch_loss += loss\n",
    "\n",
    "                # Get predictions\n",
    "                preds = output.argmax(dim=-1)\n",
    "                preds_np = preds.cpu().numpy().flatten()\n",
    "                true_np = batch_labels[patch].cpu().numpy().flatten()\n",
    "\n",
    "                # Only keep entries where the ground truth is not 255\n",
    "                valid_mask = true_np != 255\n",
    "\n",
    "                # Filter predictions and ground truths\n",
    "                filtered_preds = preds_np[valid_mask]\n",
    "                filtered_true = true_np[valid_mask]\n",
    "\n",
    "                # Append the filtered results\n",
    "                all_preds.extend(filtered_preds)\n",
    "                total_preds.extend(filtered_preds)\n",
    "                all_true_labels.extend(filtered_true)\n",
    "                total_true_labels.extend(filtered_true)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Normalize batch_loss by valid pixels\n",
    "            if len(batch_expert_patches) - count > 0:\n",
    "                batch_loss /= (len(batch_expert_patches) - count)\n",
    "                loss_per_batch.append(batch_loss.item())  # Storing batch loss only if valid\n",
    "                loss_epoch += batch_loss.item()\n",
    "                data_len += 1  # Increment count of valid batches\n",
    "\n",
    "    # Compute metrics\n",
    "    epoch_loss = loss_epoch / data_len if data_len > 0 else 0  # Avoid division by zero\n",
    "    loss_per_epoch.append(epoch_loss)\n",
    "\n",
    "    accuracy = accuracy_score(np.array(all_true_labels).flatten(), np.array(all_preds).flatten())\n",
    "    f1_micro = f1_score(np.array(all_true_labels).flatten(), np.array(all_preds).flatten(), average='micro')\n",
    "    f1_macro = f1_score(np.array(all_true_labels).flatten(), np.array(all_preds).flatten(), average='macro')\n",
    "    precision = precision_score(np.array(all_true_labels).flatten(), np.array(all_preds).flatten(), average='macro', zero_division=0)\n",
    "    recall = recall_score(np.array(all_true_labels).flatten(), np.array(all_preds).flatten(), average='macro', zero_division=0)\n",
    "    iou = jaccard_index(torch.tensor(all_preds),torch.tensor(all_true_labels), task=\"multiclass\", num_classes=3)\n",
    "\n",
    "    # Logging\n",
    "    log_file = \"mar28_exp_battery/training_log.txt\"\n",
    "    with open(log_file, \"a\") as f:\n",
    "        log_message = (f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, \"\n",
    "                       f\"Accuracy: {accuracy:.4f}, F1 Micro: {f1_micro:.4f}, F1 Macro: {f1_macro:.4f}, \"\n",
    "                       f\"Precision: {precision:.4f}, Recall: {recall:.4f}, IoU: {iou:.4f}\\n\"\n",
    "                       f\"Number of patches with no valid pixels: {num_left_count}\\n\\n\")\n",
    "        # print(log_message, end=\"\")  # Print to console\n",
    "        f.write(log_message)  # Write to file\n",
    "\n",
    "    # Save checkpoint\n",
    "    # Save checkpoint only every 10 epochs\n",
    "    if epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, accuracy: {accuracy:.4f}, F1 Micro: {f1_micro:.4f}, F1 Macro: {f1_macro:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, IoU: {iou:.4f}\")\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, accuracy: {accuracy:.4f}, F1 Micro: {f1_micro:.4f}, F1 Macro: {f1_macro:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, IoU: {iou:.4f}\")\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss  # Store averaged loss\n",
    "        }\n",
    "        torch.save(checkpoint, f'mar28_exp_battery/checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "\n",
    "# Save losses\n",
    "import pickle\n",
    "with open(\"mar28_exp_battery/loss_per_epoch_100.pkl\", \"wb\") as file:\n",
    "    pickle.dump(loss_per_epoch, file)\n",
    "with open(\"mar28_exp_battery/losses_100.pkl\", \"wb\") as file:\n",
    "    pickle.dump(losses, file)\n",
    "with open(\"mar28_exp_battery/total_true_labels_100.pkl\", \"wb\") as file:\n",
    "    pickle.dump(total_true_labels, file)\n",
    "with open(\"mar28_exp_battery/total_preds_100.pkl\", \"wb\") as file:\n",
    "    pickle.dump(total_preds, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
